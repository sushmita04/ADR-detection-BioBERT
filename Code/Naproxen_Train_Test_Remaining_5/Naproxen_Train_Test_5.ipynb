{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naproxen_Train_Test_5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Required Packages/Libraries and Importing Them"
      ],
      "metadata": {
        "id": "NiFc-4az-wEW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCT4xcKKzJac",
        "outputId": "e84cf6ab-ba9b-4a31-9d2b-6381b9ca1584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 12.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 32.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n",
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 10.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.11.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.22.3-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 18.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.2.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting botocore<1.26.0,>=1.25.3\n",
            "  Downloading botocore-1.25.3-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 33.3 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.3->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.26.0,>=1.25.3->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 36.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.22.3 botocore-1.25.3 jmespath-1.0.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.2 urllib3-1.25.11\n",
            "Collecting text-preprocessing\n",
            "  Downloading text_preprocessing-0.1.0-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from text-preprocessing) (3.2.5)\n",
            "Collecting unittest-xml-reporting\n",
            "  Downloading unittest_xml_reporting-3.2.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting names-dataset\n",
            "  Downloading names-dataset-3.0.3.tar.gz (58.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 58.4 MB 89 kB/s \n",
            "\u001b[?25hCollecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.6.3-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 15.7 MB/s \n",
            "\u001b[?25hCollecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 38.7 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 38.6 MB/s \n",
            "\u001b[?25hCollecting pycountry\n",
            "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 24.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->text-preprocessing) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pycountry->names-dataset->text-preprocessing) (57.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from unittest-xml-reporting->text-preprocessing) (4.2.6)\n",
            "Building wheels for collected packages: names-dataset, pycountry\n",
            "  Building wheel for names-dataset (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for names-dataset: filename=names_dataset-3.0.3-py3-none-any.whl size=116832511 sha256=b0a2580dcf483900718f149bcf85f2b5926ab99de5b679851524dd3b8ae4cc59\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/24/3c/4ed2ffe782079f67adaed4399351b1407eba3f475e7176d45f\n",
            "  Building wheel for pycountry (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681845 sha256=d5229b8bd3789e2f82cbe092b7fd7643b93f4bcce899af83d64fc55a968739e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/06/e8/7ee176e95ea9a8a8c3b3afcb1869f20adbd42413d4611c6eb4\n",
            "Successfully built names-dataset pycountry\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, pycountry, unittest-xml-reporting, pyspellchecker, names-dataset, contractions, text-preprocessing\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 names-dataset-3.0.3 pyahocorasick-1.4.4 pycountry-22.3.5 pyspellchecker-0.6.3 text-preprocessing-0.1.0 textsearch-0.0.21 unittest-xml-reporting-3.2.0\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-pretrained-bert\n",
        "!pip install text-preprocessing\n",
        "!pip install tweet-preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/naver/biobert-pretrained/releases/download/v1.0-pubmed-pmc/biobert_v1.0_pubmed_pmc.tar.gz\n",
        "!tar -xzf biobert_v1.0_pubmed_pmc.tar.gz\n",
        "!transformers-cli convert --model_type bert --tf_checkpoint biobert_v1.0_pubmed_pmc/biobert_model.ckpt --config biobert_v1.0_pubmed_pmc/bert_config.json --pytorch_dump_output biobert_v1.0_pubmed_pmc/pytorch_model.bin\n",
        "\n",
        "!mv biobert_v1.0_pubmed_pmc/bert_config.json biobert_v1.0_pubmed_pmc/config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL4lvnfvzLRg",
        "outputId": "750c5ef6-3248-4c6a-e9d3-6d80747a46e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-28 20:50:42--  https://github.com/naver/biobert-pretrained/releases/download/v1.0-pubmed-pmc/biobert_v1.0_pubmed_pmc.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/167883658/d9fd1200-7d44-11e9-90e0-521fb735d8fd?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220428%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220428T205042Z&X-Amz-Expires=300&X-Amz-Signature=5949fc5fff779e280e67c6c40672eb1b9786dbdb47bfbd67cc49343c959aa8c6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=167883658&response-content-disposition=attachment%3B%20filename%3Dbiobert_v1.0_pubmed_pmc.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-04-28 20:50:42--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/167883658/d9fd1200-7d44-11e9-90e0-521fb735d8fd?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220428%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220428T205042Z&X-Amz-Expires=300&X-Amz-Signature=5949fc5fff779e280e67c6c40672eb1b9786dbdb47bfbd67cc49343c959aa8c6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=167883658&response-content-disposition=attachment%3B%20filename%3Dbiobert_v1.0_pubmed_pmc.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 402016728 (383M) [application/octet-stream]\n",
            "Saving to: ‘biobert_v1.0_pubmed_pmc.tar.gz’\n",
            "\n",
            "biobert_v1.0_pubmed 100%[===================>] 383.39M  14.7MB/s    in 17s     \n",
            "\n",
            "2022-04-28 20:51:00 (23.0 MB/s) - ‘biobert_v1.0_pubmed_pmc.tar.gz’ saved [402016728/402016728]\n",
            "\n",
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Converting TensorFlow checkpoint from /content/biobert_v1.0_pubmed_pmc/biobert_model.ckpt\n",
            "Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
            "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "Loading TF weight bert/embeddings/word_embeddings with shape [28996, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/pooler/dense/bias with shape [768]\n",
            "Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "Save PyTorch model to biobert_v1.0_pubmed_pmc/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-pretrained-bert\n",
        "!pip install text-preprocessing\n",
        "!pip install tweet-preprocessor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBJCzQrLzbe6",
        "outputId": "e64ed98e-b796-48f8-a65c-0cdbd8b68dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.22.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.2.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.0.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.5.2)\n",
            "Requirement already satisfied: botocore<1.26.0,>=1.25.3 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.25.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.3->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.3->boto3->pytorch-pretrained-bert) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.26.0,>=1.25.3->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.10.8)\n",
            "Requirement already satisfied: text-preprocessing in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: unittest-xml-reporting in /usr/local/lib/python3.7/dist-packages (from text-preprocessing) (3.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from text-preprocessing) (3.2.5)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (from text-preprocessing) (0.1.72)\n",
            "Requirement already satisfied: names-dataset in /usr/local/lib/python3.7/dist-packages (from text-preprocessing) (3.0.3)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.7/dist-packages (from text-preprocessing) (0.6.3)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions->text-preprocessing) (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions->text-preprocessing) (1.4.4)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions->text-preprocessing) (0.3.1)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.7/dist-packages (from names-dataset->text-preprocessing) (22.3.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->text-preprocessing) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pycountry->names-dataset->text-preprocessing) (57.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from unittest-xml-reporting->text-preprocessing) (4.2.6)\n",
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys,math\n",
        "import itertools\n",
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "swords = stopwords.words(\"english\")\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader,SequentialSampler\n",
        "from collections import defaultdict, OrderedDict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import random\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "rWn9IjsVzfa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f53a041-1d39-4a78-d85d-d21a55ffaa3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Path is the location in google drive where the required csv files are stored."
      ],
      "metadata": {
        "id": "NQ9mtegK_AH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/Project/Naproxen/'\n",
        "#path = '/content/drive/MyDrive/Datasets (1)/indian_vaccine/'\n",
        "destination_folder = path\n",
        "tokenizer = BertTokenizer(vocab_file = 'biobert_v1.0_pubmed_pmc/vocab.txt', do_lower_case=False)"
      ],
      "metadata": {
        "id": "ocit3CtszkM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a10f9d4-ed8f-4ade-f4d9-b8358f1d6ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meddra_all_se = pd.read_csv(path+'meddra_all_se.tsv',sep='\\t',\\\n",
        "                                           names=['1','2','3','4','5','adr'])"
      ],
      "metadata": {
        "id": "5_O2DDISz6dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rlist = []\n",
        "all_adrs = []\n",
        "#all_adrs = meddra_all_label_se['adr'].str.lower().unique().tolist()\n",
        "all_adrs = all_adrs + meddra_all_se['adr'].str.lower().unique().tolist()\n",
        "\n",
        "print(len(all_adrs))\n",
        "all_adrs = np.array(all_adrs)\n",
        "all_adrs = np.unique(all_adrs)\n",
        "all_adrs = all_adrs.tolist()\n",
        "print(len(all_adrs))\n",
        "all_adrs = list(set(all_adrs) - set(rlist))\n",
        "print(len(all_adrs))"
      ],
      "metadata": {
        "id": "3QmubRwGz-AP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b3c9378-664f-48c2-cc45-04a9a0507b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6123\n",
            "6123\n",
            "6123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility functions to preprocess text and find ADRS (Used in both Training and Testing)"
      ],
      "metadata": {
        "id": "V6fIDbCe_wz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To clean your review data\n",
        "def preprocesstext(x):\n",
        "    x = x.lower()\n",
        "    x = re.sub(r'http\\S+', '', x)  #removes urls\n",
        "    x = re.sub(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\",'', x)   #remove emails\n",
        "    x = re.sub('@[^\\s]+','',x)\n",
        "    x = re.sub(r'[^\\w\\s]', '', x)                     #removing punctuations\n",
        "    x = re.sub('[^A-Za-z0-9]+', ' ', x)\n",
        "    #x = ' '.join( [w for w in x.split() if len(w)>1] )       #removing single characters\n",
        "    #x = ' '.join([word for word in x.split() if word not in swords])\n",
        "    return x\n",
        "'''\n",
        "# This function check whether a review contians ADR term or not.\n",
        "# If present returns 1 else returns 0\n",
        "'''\n",
        "def is_adrs_present(x):\n",
        "    res = []\n",
        "    for adr in all_adrs:\n",
        "        adr = re.sub(r'[^\\w\\s]', '', adr)\n",
        "        adr = re.compile(r'\\b'+adr+r'\\b')\n",
        "        match = re.search(adr, x)\n",
        "        if match != None:\n",
        "            res.append(match.group())\n",
        "            break\n",
        "    res = np.array(res)\n",
        "    res = np.unique(res)\n",
        "    res = res.tolist()\n",
        "    return 1 if bool(res) else 0\n",
        "\n",
        "'''\n",
        "This functions extracts all present ADRs and returns them\n",
        "'''\n",
        "def find_adrs(x):\n",
        "    res = []\n",
        "    for adr in all_adrs:\n",
        "        adr = re.sub(r'[^\\w\\s]', '', adr)\n",
        "        adr = re.compile(r'\\b'+adr+r'\\b')\n",
        "        match = re.search(adr, x)\n",
        "        if match != None:\n",
        "            res.append(match.group())\n",
        "    res = np.array(res)\n",
        "    res = np.unique(res)\n",
        "    res = str(res.tolist())\n",
        "    res = re.sub(r'[^\\w\\s,]', '', res)\n",
        "    #print(res)\n",
        "    return res"
      ],
      "metadata": {
        "id": "uo6Gc3DV0Ech"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the needed csv files into a pandas dataframe"
      ],
      "metadata": {
        "id": "-ZVH-mtV_7jC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use this section if you want to load a single file into the dataframe"
      ],
      "metadata": {
        "id": "ZmwDICgNheT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('AW_naproxen_processed.csv')"
      ],
      "metadata": {
        "id": "9OCZyziOiLB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "lLW172S1AgbF",
        "outputId": "1b151ba0-37df-4bfc-9602-27367e1205a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0                   Date       Star  \\\n",
              "0             0    4/1/2022 8:16:30 AM    2 Stars   \n",
              "1             1    4/1/2022 4:45:49 AM    5 Stars   \n",
              "2             2  3/21/2022 10:51:45 AM    5 Stars   \n",
              "3             3   3/8/2022 12:30:36 AM    5 Stars   \n",
              "4             4    3/6/2022 4:42:15 AM    1 Stars   \n",
              "..          ...                    ...        ...   \n",
              "206         206  10/9/2007 10:06:02 PM  4.5 Stars   \n",
              "207         207  8/30/2007 10:52:48 AM  4.5 Stars   \n",
              "208         208    8/6/2007 1:41:22 AM    5 Stars   \n",
              "209         209   6/12/2007 4:49:02 AM  4.5 Stars   \n",
              "210         210   5/29/2007 3:10:52 AM  0.5 Stars   \n",
              "\n",
              "                                             Condition  \\\n",
              "0                        Rated  for Depression Report    \n",
              "1               Rated  for Bipolar II disorder Report    \n",
              "2                        Rated  for Depression Report    \n",
              "3               Rated  for Bipolar II disorder Report    \n",
              "4                        Rated  for Depression Report    \n",
              "..                                                 ...   \n",
              "206                            Rated  for Pain Report    \n",
              "207  Rated  for Bursitis- trochanteric (Hip bursiti...   \n",
              "208                  Rated  for Osteoarthritis Report    \n",
              "209                            Rated  for Pain Report    \n",
              "210                        Rated  for Headache Report    \n",
              "\n",
              "                                                Rating  \\\n",
              "0    i woke up in so much pain after 23 days on lam...   \n",
              "1    this could be one of the best drugs ever creat...   \n",
              "2    i was diagnosed with bipolar nos and i take th...   \n",
              "3    been on lamictal for a few months after being ...   \n",
              "4    so i was recommended to try this for depressio...   \n",
              "..                                                 ...   \n",
              "206  i got slight stomach problems but was feeling ...   \n",
              "207  lower back pain and hip pain was relieved quic...   \n",
              "208  i have stage i and ii osteoarthritis and i hav...   \n",
              "209  it really helps my knee painsome stomach irrit...   \n",
              "210          hasnt really work on me still having pain   \n",
              "\n",
              "                                            adr  label  \n",
              "0    inflammation, osteoarthritis, pain, trauma      1  \n",
              "1                                           NaN      0  \n",
              "2                                           NaN      0  \n",
              "3                                           NaN      0  \n",
              "4           anxiety, crying, depression, stress      1  \n",
              "..                                          ...    ...  \n",
              "206                                         NaN      0  \n",
              "207                             back pain, pain      1  \n",
              "208                        osteoarthritis, pain      1  \n",
              "209                                         NaN      0  \n",
              "210                                        pain      1  \n",
              "\n",
              "[2407 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5ea126b3-5d1c-4a73-ac2a-edee9f338494\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Date</th>\n",
              "      <th>Star</th>\n",
              "      <th>Condition</th>\n",
              "      <th>Rating</th>\n",
              "      <th>adr</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4/1/2022 8:16:30 AM</td>\n",
              "      <td>2 Stars</td>\n",
              "      <td>Rated  for Depression Report</td>\n",
              "      <td>i woke up in so much pain after 23 days on lam...</td>\n",
              "      <td>inflammation, osteoarthritis, pain, trauma</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4/1/2022 4:45:49 AM</td>\n",
              "      <td>5 Stars</td>\n",
              "      <td>Rated  for Bipolar II disorder Report</td>\n",
              "      <td>this could be one of the best drugs ever creat...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3/21/2022 10:51:45 AM</td>\n",
              "      <td>5 Stars</td>\n",
              "      <td>Rated  for Depression Report</td>\n",
              "      <td>i was diagnosed with bipolar nos and i take th...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3/8/2022 12:30:36 AM</td>\n",
              "      <td>5 Stars</td>\n",
              "      <td>Rated  for Bipolar II disorder Report</td>\n",
              "      <td>been on lamictal for a few months after being ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3/6/2022 4:42:15 AM</td>\n",
              "      <td>1 Stars</td>\n",
              "      <td>Rated  for Depression Report</td>\n",
              "      <td>so i was recommended to try this for depressio...</td>\n",
              "      <td>anxiety, crying, depression, stress</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>206</td>\n",
              "      <td>10/9/2007 10:06:02 PM</td>\n",
              "      <td>4.5 Stars</td>\n",
              "      <td>Rated  for Pain Report</td>\n",
              "      <td>i got slight stomach problems but was feeling ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>207</td>\n",
              "      <td>8/30/2007 10:52:48 AM</td>\n",
              "      <td>4.5 Stars</td>\n",
              "      <td>Rated  for Bursitis- trochanteric (Hip bursiti...</td>\n",
              "      <td>lower back pain and hip pain was relieved quic...</td>\n",
              "      <td>back pain, pain</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>208</td>\n",
              "      <td>8/6/2007 1:41:22 AM</td>\n",
              "      <td>5 Stars</td>\n",
              "      <td>Rated  for Osteoarthritis Report</td>\n",
              "      <td>i have stage i and ii osteoarthritis and i hav...</td>\n",
              "      <td>osteoarthritis, pain</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>209</td>\n",
              "      <td>6/12/2007 4:49:02 AM</td>\n",
              "      <td>4.5 Stars</td>\n",
              "      <td>Rated  for Pain Report</td>\n",
              "      <td>it really helps my knee painsome stomach irrit...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>210</td>\n",
              "      <td>5/29/2007 3:10:52 AM</td>\n",
              "      <td>0.5 Stars</td>\n",
              "      <td>Rated  for Headache Report</td>\n",
              "      <td>hasnt really work on me still having pain</td>\n",
              "      <td>pain</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2407 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ea126b3-5d1c-4a73-ac2a-edee9f338494')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5ea126b3-5d1c-4a73-ac2a-edee9f338494 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5ea126b3-5d1c-4a73-ac2a-edee9f338494');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nePS_2NJANEz",
        "outputId": "4bd01964-faa9-49e0-c64d-b1db1baf1dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2407, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting the ADRS in the given text reviews. Change the 'Rating' column to any column that contains the text reviews as per requirements.\n",
        "### Please note that if ADRS have already been detected then this step is optional"
      ],
      "metadata": {
        "id": "Twz6RA-vAJAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.Rating=df.Rating.astype(str)\n",
        "df['Rating'] = df['Rating'].apply(preprocesstext)\n",
        "#extracting ADRs\n",
        "df['adr'] = df['Rating'].apply(find_adrs)\n",
        "print(type(df['adr'].iloc[0]))"
      ],
      "metadata": {
        "id": "v8Or7Cv60oR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c71a39-ba13-4ca3-8e25-8bb7ad507f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = df['adr'].apply(is_adrs_present)\n",
        "df"
      ],
      "metadata": {
        "id": "giHr4ZrB0rgg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "b0237579-80d3-4198-dd35-13046fa0236a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       Date       Star  \\\n",
              "0       4/1/2022 8:16:30 AM    2 Stars   \n",
              "1       4/1/2022 4:45:49 AM    5 Stars   \n",
              "2     3/21/2022 10:51:45 AM    5 Stars   \n",
              "3      3/8/2022 12:30:36 AM    5 Stars   \n",
              "4       3/6/2022 4:42:15 AM    1 Stars   \n",
              "...                     ...        ...   \n",
              "1249   6/21/2002 2:11:37 AM    1 Stars   \n",
              "1250   6/12/2002 8:13:47 PM  4.5 Stars   \n",
              "1251    6/7/2002 6:02:30 PM    5 Stars   \n",
              "1252   4/28/2002 2:11:38 PM  4.5 Stars   \n",
              "1253    2/5/2002 2:03:11 AM  1.5 Stars   \n",
              "\n",
              "                                   Condition  \\\n",
              "0              Rated  for Depression Report    \n",
              "1     Rated  for Bipolar II disorder Report    \n",
              "2              Rated  for Depression Report    \n",
              "3     Rated  for Bipolar II disorder Report    \n",
              "4              Rated  for Depression Report    \n",
              "...                                      ...   \n",
              "1249           Rated  for Depression Report    \n",
              "1250  Rated  for Bipolar II disorder Report    \n",
              "1251  Rated  for Bipolar II disorder Report    \n",
              "1252  Rated  for Bipolar II disorder Report    \n",
              "1253   Rated  for Bipolar I disorder Report    \n",
              "\n",
              "                                                 Rating  \\\n",
              "0     i woke up in so much pain after 23 days on lam...   \n",
              "1     this could be one of the best drugs ever creat...   \n",
              "2     i was diagnosed with bipolar nos and i take th...   \n",
              "3     been on lamictal for a few months after being ...   \n",
              "4     so i was recommended to try this for depressio...   \n",
              "...                                                 ...   \n",
              "1249  lamictal side effects after trying several med...   \n",
              "1250  ongoing lamictal i combine lamictal 100mg2xday...   \n",
              "1251  lamictal just diagnosed with bipolar after man...   \n",
              "1252  lamictal works well for me i take 600mg of lam...   \n",
              "1253  lamictal i had to go to the emergency room wit...   \n",
              "\n",
              "                                             adr  label  \n",
              "0     inflammation, osteoarthritis, pain, trauma      1  \n",
              "1                                                     0  \n",
              "2                                                     0  \n",
              "3                                                     0  \n",
              "4            anxiety, crying, depression, stress      1  \n",
              "...                                          ...    ...  \n",
              "1249                      acne, depression, rash      1  \n",
              "1250                                     anxiety      1  \n",
              "1251                                                  0  \n",
              "1252                                  depression      1  \n",
              "1253                                        rash      1  \n",
              "\n",
              "[1254 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13e6b3a2-773d-4070-8cbc-542693b95b75\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Star</th>\n",
              "      <th>Condition</th>\n",
              "      <th>Rating</th>\n",
              "      <th>adr</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4/1/2022 8:16:30 AM</td>\n",
              "      <td>2 Stars</td>\n",
              "      <td>Rated  for Depression Report</td>\n",
              "      <td>i woke up in so much pain after 23 days on lam...</td>\n",
              "      <td>inflammation, osteoarthritis, pain, trauma</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4/1/2022 4:45:49 AM</td>\n",
              "      <td>5 Stars</td>\n",
              "      <td>Rated  for Bipolar II disorder Report</td>\n",
              "      <td>this could be one of the best drugs ever creat...</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3/21/2022 10:51:45 AM</td>\n",
              "      <td>5 Stars</td>\n",
              "      <td>Rated  for Depression Report</td>\n",
              "      <td>i was diagnosed with bipolar nos and i take th...</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3/8/2022 12:30:36 AM</td>\n",
              "      <td>5 Stars</td>\n",
              "      <td>Rated  for Bipolar II disorder Report</td>\n",
              "      <td>been on lamictal for a few months after being ...</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3/6/2022 4:42:15 AM</td>\n",
              "      <td>1 Stars</td>\n",
              "      <td>Rated  for Depression Report</td>\n",
              "      <td>so i was recommended to try this for depressio...</td>\n",
              "      <td>anxiety, crying, depression, stress</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1249</th>\n",
              "      <td>6/21/2002 2:11:37 AM</td>\n",
              "      <td>1 Stars</td>\n",
              "      <td>Rated  for Depression Report</td>\n",
              "      <td>lamictal side effects after trying several med...</td>\n",
              "      <td>acne, depression, rash</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250</th>\n",
              "      <td>6/12/2002 8:13:47 PM</td>\n",
              "      <td>4.5 Stars</td>\n",
              "      <td>Rated  for Bipolar II disorder Report</td>\n",
              "      <td>ongoing lamictal i combine lamictal 100mg2xday...</td>\n",
              "      <td>anxiety</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1251</th>\n",
              "      <td>6/7/2002 6:02:30 PM</td>\n",
              "      <td>5 Stars</td>\n",
              "      <td>Rated  for Bipolar II disorder Report</td>\n",
              "      <td>lamictal just diagnosed with bipolar after man...</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1252</th>\n",
              "      <td>4/28/2002 2:11:38 PM</td>\n",
              "      <td>4.5 Stars</td>\n",
              "      <td>Rated  for Bipolar II disorder Report</td>\n",
              "      <td>lamictal works well for me i take 600mg of lam...</td>\n",
              "      <td>depression</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1253</th>\n",
              "      <td>2/5/2002 2:03:11 AM</td>\n",
              "      <td>1.5 Stars</td>\n",
              "      <td>Rated  for Bipolar I disorder Report</td>\n",
              "      <td>lamictal i had to go to the emergency room wit...</td>\n",
              "      <td>rash</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1254 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13e6b3a2-773d-4070-8cbc-542693b95b75')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-13e6b3a2-773d-4070-8cbc-542693b95b75 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-13e6b3a2-773d-4070-8cbc-542693b95b75');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the text reviews with ADRS. Change the file-name as per requirements"
      ],
      "metadata": {
        "id": "PYM7FU-sArHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(path+'AW_naproxen_processed.csv')"
      ],
      "metadata": {
        "id": "35F4lua20t4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If text has already been preprocessed and ADRS have been generated, kindly run the code below. Change \"Rating\" to match the name of the column containing text reviews as per requirements."
      ],
      "metadata": {
        "id": "yICcMHRlCbhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.astype({\"adr\": str})\n",
        "df = df.astype({\"Rating\": str})\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83Aqen4fAwEh",
        "outputId": "61c63e5c-099e-402b-f918-30d9e71a66e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0     int64\n",
              "Date          object\n",
              "Star          object\n",
              "Condition     object\n",
              "Rating        object\n",
              "adr           object\n",
              "label          int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions to get the BIO-sequence labeling\n",
        "### Please change the 'Rating' value in get_tags to match the name of the column in your file containing the text data."
      ],
      "metadata": {
        "id": "fjDsp1OyB6Jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bio_taggin(x,adrs,label):\n",
        "    #texts = re.findall(r\"[A-Za-z@#]+|\\S\", x) #splits each word but splits even 1st to '1' and 'st'\n",
        "    texts = x.split()                        #splits each word but keeps 1st as '1st'\n",
        "    if len(adrs)==0 or label==0:        \n",
        "        tags = ['O']*len(texts)\n",
        "        #df = pd.DataFrame(list(zip(texts, tags)),columns=['text','tag'])\n",
        "        #print(df)\n",
        "    else:\n",
        "        tags = ['O']*len(texts)\n",
        "        adrs = adrs.split(',')\n",
        "        adrs = sorted(adrs,key=len)\n",
        "        print(adrs)\n",
        "        for adr in adrs:\n",
        "            alist = adr.split()\n",
        "            #print(alist)\n",
        "            for i in range(0,len(alist)):\n",
        "                for j in range(0,len(texts)):\n",
        "                    if texts[j] == str(alist[i]):\n",
        "                        if i == 0 and tags[j]=='O':\n",
        "                            tags[j] = 'B'\n",
        "                        if i >= 1 and (tags[j-1]=='B' or tags[j-1]=='I'):\n",
        "                          tags[j] = 'I'\n",
        "        #df = pd.DataFrame(list(zip(texts, tags)),columns=['text','tag'])\n",
        "    return list(texts),list(tags)\n",
        "\n",
        "\n",
        "# Labelling BIO tags to sentence words\n",
        "def get_tags(df):\n",
        "  sentences = []\n",
        "  tags = []\n",
        "\n",
        "  for index,row in df.iterrows():\n",
        "      sent,tag = bio_taggin(row['Rating'], row['adr'],row['label'])\n",
        "      \n",
        "      sentences.append(sent)\n",
        "      tags.append(tag)\n",
        "  return sentences, tags\n",
        "\n",
        "def tok_with_labels(sent, text_labels):\n",
        "  '''tokenize and keep labels intact'''\n",
        "  tok_sent = []\n",
        "  labels = []\n",
        "  for word, label in zip(sent, text_labels):\n",
        "    tok_word = tokenizer.tokenize(word)\n",
        "    n_subwords = len(tok_word)\n",
        "\n",
        "    tok_sent.extend(tok_word)\n",
        "    labels.extend([label] * n_subwords)\n",
        "  return tok_sent, labels"
      ],
      "metadata": {
        "id": "7abcEMG90wGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAX_LEN refers to the max number of wordpiece tokens in a review. Here, used value is 120. batch_size refers to the batch_size in the sequential dataloader used for training and testing\n",
        "\n"
      ],
      "metadata": {
        "id": "DNRf0tPrC4K3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 120   #120\n",
        "batch_size = 16\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "GYvHTtPp0y4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a11d5f-c25b-4e9d-cbbb-1634c4615920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtaining Input IDs, Attention Masks and Tags for the given dataframe."
      ],
      "metadata": {
        "id": "9SqOGV98DoWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting individual sentences and its corresponding tags\n",
        "train_sentences, train_tags = get_tags(df)\n",
        "\n",
        "# Getting tokenized texts andn labels\n",
        "# Each element is a tuple of : i) sentence in tokens and ii) BIO tags for each token\n",
        "tok_texts_and_labels = [tok_with_labels(sent, labs) for sent, labs in zip(train_sentences, train_tags)]\n",
        "\n",
        "# Separating tokens and labels (BIO tags)\n",
        "tok_texts = [tok_label_pair[0] for tok_label_pair in tok_texts_and_labels]\n",
        "labels = [tok_label_pair[1] for tok_label_pair in tok_texts_and_labels]\n",
        "'''\n",
        "1. Making Input ids\n",
        "2. Making Labels ids\n",
        "3. Making attention masks\n",
        "'''\n",
        "# 1. Making sentences input ids\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tok_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# 2. Making BIO tag input ids\n",
        "tag_values = ['O', 'B', 'I', 'PAD']\n",
        "#tag_values = list(set(itertools.chain.from_iterable(train_tags)))\n",
        "vocab_len = len(tag_values)\n",
        "tag2idx =  {'O': 0, 'B': 1, 'I': 2, 'PAD': 3}\n",
        "#tag2idx = {t: i for i,t in enumerate(tag_values)}\n",
        "\n",
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "\n",
        "# 3. Making attention masks\n",
        "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n"
      ],
      "metadata": {
        "id": "tZYJCfJO01f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Printing the Rating, tokenized text, input ids, tags and attention masks for one particular row"
      ],
      "metadata": {
        "id": "YBb1Mpd3D-DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Rating'].iloc[0])\n",
        "print(tok_texts[0])\n",
        "print(input_ids[0])\n",
        "print(tags[0])"
      ],
      "metadata": {
        "id": "9gg6SQzR1UDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db054737-a6d6-4184-abec-274ac673ea38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lamictal is an important component of my medication cocktail it helps relieve my tendency to go up and down in moods it is not enough to fix my depression on its own but i think it is helpful i have not experienced side effects with this medication\n",
            "['i', 'woke', 'up', 'in', 'so', 'much', 'pain', 'after', '23', 'days', 'on', 'la', '##mot', '##rig', '##ine', 'at', '25', 'mg', 'all', '##over', 'body', 'ache', '##s', 'my', 'low', 'back', 'and', 'especially', 'painful', 'almost', 'im', '##mo', '##bil', '##izing', 'were', 'the', 'areas', 'i', 'already', 'experience', 'daily', 'pain', 'from', 'o', '##ste', '##oar', '##th', '##rit', '##is', 'i', 'felt', 'like', 'id', 'been', 'beaten', 'in', 'my', 'sleep', 'on', 'top', 'of', 'that', 'pain', 'sometimes', 'i', 'doubt', 'my', 'experience', 'and', 'judgment', '##s', 'but', 'this', 'was', 'und', '##enia', '##ble', 'and', 'very', 'unusual', 'for', 'me', 'i', 'took', 'the', 'pill', 'for', 'one', 'more', 'day', 'and', 'the', 'pain', 'was', 'even', 'worse', 'so', 'i', 'stopped', 'on', 'the', '5th', 'day', 'for', 'the', 'entire', 'weekend', 'on', 'mon', '##day', 'my', 'doctor', 'said', 'this', 'wasn', '##t', 'a', 'common', 'side', 'effect', 'and', 'probably', 'unrelated', 'why', 'im', 'adding', 'this', 'review', 'so', 'giving', 'it', 'a', 'good', 'try', 'i', 'started', 'again', 'that', 'day', 'and', 'hoped', 'to', 'narrow', 'this', 'down', 'to', 'the', 'medicine', 'it', 'was', 'slightly', 'less', 'painful', 'but', 'within', '4', 'days', 'i', 'had', 'a', 'lot', 'of', 'pain', 'again', 'i', 'talked', 'to', 'my', 'doctor', 'today', 'and', 'said', 'maybe', 'this', 'r', '##x', 'trigger', '##s', 'inflammation', 'in', 'my', 'body', 'so', 'for', 'what', 'its', 'worth', 'la', '##mot', '##rig', '##ine', 'didn', '##t', 'work', 'for', 'me', 'in', 'fact', 'it', 'made', 'my', 'life', 'worse', 'for', 'those', 'two', 'weeks', 'and', 'with', 'my', 'm', '##dd', '##de', '##press', '##ion', 'and', 'complex', 'trauma', 'symptoms', 'more', 'trouble', 'is', 'the', 'very', 'last', 'thing', 'i', 'need', 'in', 'the', 'world', 'i', 'may', 'or', 'may', 'not', 'try', 'another', 'mood', 'stab', '##ilizer', 'in', 'the', 'future', 'im', 'wary', 'to', 'say', 'the', 'least']\n",
            "[  178  7994  1146  1107  1177  1277  2489  1170  1695  1552  1113  2495\n",
            " 21277 17305  2042  1120  1512 17713  1155  5909  1404 12953  1116  1139\n",
            "  1822  1171  1105  2108  8920  1593 13280  3702 15197  4404  1127  1103\n",
            "  1877   178  1640  2541  3828  2489  1121   184 13894 19243  1582  7729\n",
            "  1548   178  1464  1176 25021  1151  7425  1107  1139  2946  1113  1499\n",
            "  1104  1115  2489  2121   178  4095  1139  2541  1105  9228  1116  1133\n",
            "  1142  1108  5576 23179  2165  1105  1304  5283  1111  1143   178  1261\n",
            "  1103 21822  1111  1141  1167  1285  1105  1103  2489  1108  1256  4146\n",
            "  1177   178  2141  1113  1103  4025  1285  1111  1103  2072  5138  1113\n",
            " 19863  6194  1139  3995  1163  1142  1445  1204   170  1887  1334  2629]\n",
            "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing Train Validation Split and making dataloaders for the respective train and validation datasets"
      ],
      "metadata": {
        "id": "rjwtgfJCFTsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
        "                                                            random_state=9, test_size=0.1)\n",
        "#tr_inputs = np.concatenate((tr_inputs, val_inputs))\n",
        "#tr_tags = np.concatenate((tr_tags, val_tags))\n",
        "\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=9, test_size=0.1)\n",
        "\n",
        "#tr_masks = tr_masks + val_masks\n",
        "\n",
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "val_tags = torch.tensor(val_tags)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "metadata": {
        "id": "dNbOxOpGBlTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = SequentialSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "_gsx2_1GBnKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility functions to save and load the trained BioBERT model"
      ],
      "metadata": {
        "id": "eoHltuhYF3gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_checkpoint(load_path, model, optimizer):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "    \n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
      ],
      "metadata": {
        "id": "KmeKQV7T1bAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The main neural network."
      ],
      "metadata": {
        "id": "VqjKXBv8GLSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BioBertNER(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    #self.bert = BertModel(config)\n",
        "    #self.bert.load_state_dict(state_dict, strict=False)\n",
        "    self.bert = BertModel.from_pretrained('biobert_v1.0_pubmed_pmc')\n",
        "    self.dropout = nn.Dropout(p=0.3)\n",
        "    self.output = nn.Linear(self.bert.config.hidden_size, 4)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    encoded_layer, _ = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    encl = encoded_layer[-1]\n",
        "    #print('embedding: ', encl.shape)\n",
        "    out = self.dropout(encl)\n",
        "    out = self.output(out)\n",
        "    #print('Linear: ',out.shape)\n",
        "    return out, out.argmax(-1)"
      ],
      "metadata": {
        "id": "vKrQ7MMU1e2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility function for training one epoch"
      ],
      "metadata": {
        "id": "fW4fJ5_8GccU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model,data_loader,loss_fn,optimizer,device):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    for step,batch in enumerate(data_loader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        outputs,y_hat = model(b_input_ids,b_input_mask)\n",
        "        \n",
        "        _,preds = torch.max(outputs,dim=2)\n",
        "        \n",
        "\n",
        "        outputs = outputs.view(-1,outputs.shape[-1])\n",
        "        b_labels_shaped = b_labels.view(-1)\n",
        "        loss = loss_fn(outputs,b_labels_shaped)\n",
        "        correct_predictions += torch.sum(preds == b_labels)\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "    return correct_predictions.double()/len(data_loader) , np.mean(losses)"
      ],
      "metadata": {
        "id": "hLWn31Aw1iYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility function for evaluating the model after training for one epoch"
      ],
      "metadata": {
        "id": "seXBtoupGlem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_eval(model,data_loader,loss_fn,device):\n",
        "    model = model.eval()\n",
        "    \n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(data_loader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "            outputs,y_hat = model(b_input_ids,b_input_mask)\n",
        "        \n",
        "            _,preds = torch.max(outputs,dim=2)\n",
        "            outputs = outputs.view(-1,outputs.shape[-1])\n",
        "            b_labels_shaped = b_labels.view(-1)\n",
        "            loss = loss_fn(outputs,b_labels_shaped)\n",
        "            correct_predictions += torch.sum(preds == b_labels)\n",
        "            losses.append(loss.item())\n",
        "        \n",
        "    \n",
        "    return correct_predictions.double()/len(data_loader) , np.mean(losses)"
      ],
      "metadata": {
        "id": "L6l23HVt1mYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This cell is responsible for training the neural network. In line 5, lr and weight_decay represent learning rate and weight decay respectively. In line 6,epochs represents the number of epochs for which the model will be trained. In the last line, the value passed to save_checkpoint saves the trained network with the particular name and path in google drive."
      ],
      "metadata": {
        "id": "nBTotFbrGteL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BioBertNER().to(device)\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001,weight_decay=0.0001)\n",
        "epochs = 50\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "normalizer = batch_size * MAX_LEN\n",
        "loss_values = []\n",
        "val_loss_values = []\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    total_loss = 0\n",
        "    print(f'======== Epoch {epoch+1}/{epochs} ========')\n",
        "    train_acc,train_loss = train_epoch(model,train_dataloader,loss_fn,optimizer,device)\n",
        "    train_acc = train_acc/normalizer\n",
        "    print(f'Train Loss: {train_loss} Train Accuracy: {train_acc}')\n",
        "    total_loss += train_loss.item()\n",
        "    \n",
        "    avg_train_loss = total_loss / len(train_dataloader)  \n",
        "    loss_values.append(avg_train_loss)\n",
        "    \n",
        "    val_acc,val_loss = model_eval(model,valid_dataloader,loss_fn,device)\n",
        "    val_loss_values.append(val_loss)\n",
        "    val_acc = val_acc/normalizer\n",
        "    print(f'Val Loss: {val_loss} Val Accuracy: {val_acc}')\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    \n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    save_checkpoint(path+'seq_labelling_model_aw_naproxen.pt', model, optimizer, val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsmW0VZ3BsRn",
        "outputId": "22ff6b48-2dc7-4362-8aeb-e7c875cf97ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 1/50 ========\n",
            "Train Loss: 0.21796097603681333 Train Accuracy: 0.9183785232843138\n",
            "Val Loss: 0.04675084934569895 Val Accuracy: 0.9250651041666667\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 2/50 ========\n",
            "Train Loss: 0.038120997658766365 Train Accuracy: 0.9828010110294118\n",
            "Val Loss: 0.03331394802080467 Val Accuracy: 0.9308593749999999\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 3/50 ========\n",
            "Train Loss: 0.026537071550538874 Train Accuracy: 0.9868795955882352\n",
            "Val Loss: 0.029413575073704123 Val Accuracy: 0.9322591145833333\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 4/50 ========\n",
            "Train Loss: 0.019103632065440145 Train Accuracy: 0.9894837622549019\n",
            "Val Loss: 0.02264740865211934 Val Accuracy: 0.9353841145833334\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 5/50 ========\n",
            "Train Loss: 0.014613237139522372 Train Accuracy: 0.9910117953431372\n",
            "Val Loss: 0.018957067601149902 Val Accuracy: 0.9358723958333334\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 6/50 ========\n",
            "Train Loss: 0.011160033826907986 Train Accuracy: 0.9920343137254902\n",
            "Val Loss: 0.016965326183708385 Val Accuracy: 0.9365885416666666\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 7/50 ========\n",
            "Train Loss: 0.008926830215119374 Train Accuracy: 0.992712162990196\n",
            "Val Loss: 0.015648737862647977 Val Accuracy: 0.9375\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 8/50 ========\n",
            "Train Loss: 0.007400695814473746 Train Accuracy: 0.9932483149509804\n",
            "Val Loss: 0.015988895676855464 Val Accuracy: 0.9370768229166666\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 9/50 ========\n",
            "Train Loss: 0.00565074949982398 Train Accuracy: 0.9937385110294117\n",
            "Val Loss: 0.01618772590154549 Val Accuracy: 0.9370442708333333\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 10/50 ========\n",
            "Train Loss: 0.004126677204433637 Train Accuracy: 0.9941980698529411\n",
            "Val Loss: 0.019685013377966243 Val Accuracy: 0.9366861979166666\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 11/50 ========\n",
            "Train Loss: 0.0036513044097399175 Train Accuracy: 0.9943550857843138\n",
            "Val Loss: 0.018424528610921698 Val Accuracy: 0.9370768229166666\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 12/50 ========\n",
            "Train Loss: 0.0033786260028136894 Train Accuracy: 0.9944125306372548\n",
            "Val Loss: 0.01964617721387185 Val Accuracy: 0.9370768229166666\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 13/50 ========\n",
            "Train Loss: 0.0027051381454357933 Train Accuracy: 0.9946193321078431\n",
            "Val Loss: 0.022397370310500264 Val Accuracy: 0.937109375\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 14/50 ========\n",
            "Train Loss: 0.002418069835745401 Train Accuracy: 0.9947457107843137\n",
            "Val Loss: 0.019954854054049065 Val Accuracy: 0.9377278645833333\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 15/50 ========\n",
            "Train Loss: 0.002773512830518645 Train Accuracy: 0.9945733762254902\n",
            "Val Loss: 0.017711818174575455 Val Accuracy: 0.9380859375\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 16/50 ========\n",
            "Train Loss: 0.002170924340723158 Train Accuracy: 0.9947840073529411\n",
            "Val Loss: 0.019845987464577775 Val Accuracy: 0.93779296875\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 17/50 ========\n",
            "Train Loss: 0.0022469708131693015 Train Accuracy: 0.9947878370098039\n",
            "Val Loss: 0.019759976476052543 Val Accuracy: 0.9375\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 18/50 ========\n",
            "Train Loss: 0.00179351749305651 Train Accuracy: 0.9949027267156862\n",
            "Val Loss: 0.022242560289214452 Val Accuracy: 0.9374674479166667\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 19/50 ========\n",
            "Train Loss: 0.0013172613148794078 Train Accuracy: 0.9950712316176471\n",
            "Val Loss: 0.020394775670411036 Val Accuracy: 0.9380208333333333\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 20/50 ========\n",
            "Train Loss: 0.0017231039020557278 Train Accuracy: 0.9949640012254902\n",
            "Val Loss: 0.020935982573064393 Val Accuracy: 0.937890625\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 21/50 ========\n",
            "Train Loss: 0.0018991369669867494 Train Accuracy: 0.9949678308823529\n",
            "Val Loss: 0.01991104967191859 Val Accuracy: 0.9385416666666666\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 22/50 ========\n",
            "Train Loss: 0.0012628168228997418 Train Accuracy: 0.9950750612745097\n",
            "Val Loss: 0.02248425495599804 Val Accuracy: 0.9376302083333333\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 23/50 ========\n",
            "Train Loss: 0.0016021678706880159 Train Accuracy: 0.9949946384803922\n",
            "Val Loss: 0.018647589515239815 Val Accuracy: 0.93828125\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 24/50 ========\n",
            "Train Loss: 0.0008924445533814244 Train Accuracy: 0.9951669730392156\n",
            "Val Loss: 0.019986145553502865 Val Accuracy: 0.9381835937499999\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 25/50 ========\n",
            "Train Loss: 0.0009265166126009289 Train Accuracy: 0.9951554840686274\n",
            "Val Loss: 0.02242893893708242 Val Accuracy: 0.9376953124999999\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 26/50 ========\n",
            "Train Loss: 0.0013833348944792627 Train Accuracy: 0.9950827205882352\n",
            "Val Loss: 0.01928839962602069 Val Accuracy: 0.93779296875\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 27/50 ========\n",
            "Train Loss: 0.0011009491540522397 Train Accuracy: 0.9950980392156862\n",
            "Val Loss: 0.021127424819042062 Val Accuracy: 0.9377278645833333\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 28/50 ========\n",
            "Train Loss: 0.0012892634846032119 Train Accuracy: 0.9950367647058823\n",
            "Val Loss: 0.018451969074249064 Val Accuracy: 0.9381835937499999\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 29/50 ========\n",
            "Train Loss: 0.0014499880514503114 Train Accuracy: 0.9950291053921568\n",
            "Val Loss: 0.0216358077636869 Val Accuracy: 0.9373046875\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 30/50 ========\n",
            "Train Loss: 0.0010633545457762118 Train Accuracy: 0.9951133578431371\n",
            "Val Loss: 0.022844677522471102 Val Accuracy: 0.93740234375\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 31/50 ========\n",
            "Train Loss: 0.0013548224177986198 Train Accuracy: 0.9950022977941176\n",
            "Val Loss: 0.022793440185978397 Val Accuracy: 0.9372721354166667\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 32/50 ========\n",
            "Train Loss: 0.0010351462172847653 Train Accuracy: 0.9950980392156862\n",
            "Val Loss: 0.02251162908396509 Val Accuracy: 0.9378580729166667\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 33/50 ========\n",
            "Train Loss: 0.0006238778362732025 Train Accuracy: 0.995266544117647\n",
            "Val Loss: 0.02331222766542851 Val Accuracy: 0.9383463541666667\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 34/50 ========\n",
            "Train Loss: 0.0007706957060102414 Train Accuracy: 0.9951784620098039\n",
            "Val Loss: 0.024851046739286176 Val Accuracy: 0.9374348958333333\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 35/50 ========\n",
            "Train Loss: 0.0007993883384508937 Train Accuracy: 0.9951937806372548\n",
            "Val Loss: 0.02191731307311784 Val Accuracy: 0.9377604166666667\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 36/50 ========\n",
            "Train Loss: 0.0009101231881579545 Train Accuracy: 0.9951937806372548\n",
            "Val Loss: 0.02208639167520232 Val Accuracy: 0.93798828125\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 37/50 ========\n",
            "Train Loss: 0.0014710859062528042 Train Accuracy: 0.9949984681372548\n",
            "Val Loss: 0.02393878277780459 Val Accuracy: 0.9373697916666667\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 38/50 ========\n",
            "Train Loss: 0.0009642668788086774 Train Accuracy: 0.9951631433823529\n",
            "Val Loss: 0.024224437245720765 Val Accuracy: 0.9375\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 39/50 ========\n",
            "Train Loss: 0.0009934032584222971 Train Accuracy: 0.9951171875\n",
            "Val Loss: 0.02136310953164866 Val Accuracy: 0.93857421875\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 40/50 ========\n",
            "Train Loss: 0.0008218066932386137 Train Accuracy: 0.9951669730392156\n",
            "Val Loss: 0.01948818436721922 Val Accuracy: 0.9383138020833334\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 41/50 ========\n",
            "Train Loss: 0.0008374187074664138 Train Accuracy: 0.9951822916666666\n",
            "Val Loss: 0.01951158980227774 Val Accuracy: 0.9384114583333333\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 42/50 ========\n",
            "Train Loss: 0.0008828054687375483 Train Accuracy: 0.9951899509803921\n",
            "Val Loss: 0.023994542636501137 Val Accuracy: 0.9373372395833334\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 43/50 ========\n",
            "Train Loss: 0.0011721634201047196 Train Accuracy: 0.9951056985294117\n",
            "Val Loss: 0.027313852036968456 Val Accuracy: 0.9371744791666666\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 44/50 ========\n",
            "Train Loss: 0.0009143944797709564 Train Accuracy: 0.9951286764705881\n",
            "Val Loss: 0.023361670901067555 Val Accuracy: 0.9372395833333333\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 45/50 ========\n",
            "Train Loss: 0.001246574104350336 Train Accuracy: 0.9950022977941176\n",
            "Val Loss: 0.01915474825364072 Val Accuracy: 0.9384114583333333\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 46/50 ========\n",
            "Train Loss: 0.0005930091356009036 Train Accuracy: 0.995251225490196\n",
            "Val Loss: 0.01971703817036996 Val Accuracy: 0.93857421875\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 47/50 ========\n",
            "Train Loss: 0.0006614777030068473 Train Accuracy: 0.9952703737745098\n",
            "Val Loss: 0.022079412274706556 Val Accuracy: 0.9379557291666667\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 48/50 ========\n",
            "Train Loss: 0.0006455147587771268 Train Accuracy: 0.9952244178921569\n",
            "Val Loss: 0.01865310470066106 Val Accuracy: 0.9388020833333334\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 49/50 ========\n",
            "Train Loss: 0.0007265575193153328 Train Accuracy: 0.9951861213235293\n",
            "Val Loss: 0.021371596421772665 Val Accuracy: 0.9379231770833333\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n",
            "======== Epoch 50/50 ========\n",
            "Train Loss: 0.0011364070294214745 Train Accuracy: 0.9950788909313726\n",
            "Val Loss: 0.019787144642236854 Val Accuracy: 0.9384765625\n",
            "Model saved to ==> /content/drive/My Drive/Project/EverydayHealth/seq_labelling_model_everydayhealth.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph for Training and Validation loss as the number of epochs increases"
      ],
      "metadata": {
        "id": "EkTmMDh5H_5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style='darkgrid')\n",
        "\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# learning curve\n",
        "plt.plot(loss_values, 'b-o')\n",
        "plt.plot(val_loss_values, 'r-o')\n",
        "\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "HQFadxpkDeT3",
        "outputId": "e3abe783-330f-46a0-bf68-b94555138a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "1fCtJLTLIKtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the test datasets. Please note that the loaded datasets must have identical columns."
      ],
      "metadata": {
        "id": "4VkVvo_JMTBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_1=pd.read_csv(path+'AW_lamotrigine_processed.csv')\n",
        "df_2=pd.read_csv(path+'AW_carbamazepine_processed.csv')\n",
        "df_3=pd.read_csv(path+'AW_sumatriptan_processed.csv')\n",
        "df_4=pd.read_csv(path+'AW_levetiracetam_processed.csv')\n",
        "df_5=pd.read_csv(path+'AW_acetaminophen_processed.csv')\n",
        "df = pd.concat([df_1, df_2,df_3,df_4,df_5,df_6])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "StWI_TlTK1Gn",
        "outputId": "875543ee-bfbc-4c45-a40f-c5c93a7dc169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0                Date                   Time Taken  \\\n",
              "0             0                 NaN                          NaN   \n",
              "1             1                 NaN                          NaN   \n",
              "2             2                 NaN                          NaN   \n",
              "3             3                 NaN                          NaN   \n",
              "4             4                Kate                 June 4, 2020   \n",
              "..          ...                 ...                          ...   \n",
              "632         632     August 19, 2021      Taken for 5 to 10 years   \n",
              "633         633  September 23, 2021  Taken for less than 1 month   \n",
              "634         634    January 26, 2022       Taken for 2 to 5 years   \n",
              "635         635      March 22, 2021  Taken for less than 1 month   \n",
              "636         636    February 3, 2021                          NaN   \n",
              "\n",
              "                                        Condition  \\\n",
              "0    Lamictal (lamotrigine) for Bipolar Disorder:   \n",
              "1    Lamictal (lamotrigine) for Bipolar Disorder:   \n",
              "2    Lamictal (lamotrigine) for Bipolar Disorder:   \n",
              "3    Lamictal (lamotrigine) for Bipolar Disorder:   \n",
              "4                                    For Anxiety:   \n",
              "..                                            ...   \n",
              "632                                     For Pain:   \n",
              "633                              For Period Pain:   \n",
              "634                                     For Pain:   \n",
              "635                    Aleve (naproxen) for Pain:   \n",
              "636                              For Period Pain:   \n",
              "\n",
              "                                                Review  Rating  \\\n",
              "0    ive tried almost all meds for depressionbipola...     8.0   \n",
              "1    i have been on this drug lamictal total time o...     9.0   \n",
              "2    male 33 years old ive been on lamictal for 4 m...    10.0   \n",
              "3    i had been empty depressed doesnt justify the ...    10.0   \n",
              "4    i never review meds but i feel like im doing a...    10.0   \n",
              "..                                                 ...     ...   \n",
              "632  my dose is 500 mg twice daily for chronic pain...    10.0   \n",
              "633  worked pretty well for my heavy cramps they ar...     9.0   \n",
              "634  does not help immediate pain must take for 23 ...     2.0   \n",
              "635             this medicine does nothing for my pain     1.0   \n",
              "636  naproxen is the best thing to relieve back pai...     9.0   \n",
              "\n",
              "                                            adr  label  \n",
              "0    constipation, depression, insomnia, nausea      1  \n",
              "1                                           NaN      0  \n",
              "2                                   mood swings      1  \n",
              "3                                    depression      1  \n",
              "4                    anxiety, death, depression      1  \n",
              "..                                          ...    ...  \n",
              "632                          chronic pain, pain      1  \n",
              "633                                         NaN      0  \n",
              "634                             arthritis, pain      1  \n",
              "635                                        pain      1  \n",
              "636                             back pain, pain      1  \n",
              "\n",
              "[3478 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fb8e7ceb-091d-4adc-a9b6-99344c1d0b52\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Date</th>\n",
              "      <th>Time Taken</th>\n",
              "      <th>Condition</th>\n",
              "      <th>Review</th>\n",
              "      <th>Rating</th>\n",
              "      <th>adr</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lamictal (lamotrigine) for Bipolar Disorder:</td>\n",
              "      <td>ive tried almost all meds for depressionbipola...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>constipation, depression, insomnia, nausea</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lamictal (lamotrigine) for Bipolar Disorder:</td>\n",
              "      <td>i have been on this drug lamictal total time o...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lamictal (lamotrigine) for Bipolar Disorder:</td>\n",
              "      <td>male 33 years old ive been on lamictal for 4 m...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>mood swings</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lamictal (lamotrigine) for Bipolar Disorder:</td>\n",
              "      <td>i had been empty depressed doesnt justify the ...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>depression</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Kate</td>\n",
              "      <td>June 4, 2020</td>\n",
              "      <td>For Anxiety:</td>\n",
              "      <td>i never review meds but i feel like im doing a...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>anxiety, death, depression</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>632</th>\n",
              "      <td>632</td>\n",
              "      <td>August 19, 2021</td>\n",
              "      <td>Taken for 5 to 10 years</td>\n",
              "      <td>For Pain:</td>\n",
              "      <td>my dose is 500 mg twice daily for chronic pain...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>chronic pain, pain</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633</th>\n",
              "      <td>633</td>\n",
              "      <td>September 23, 2021</td>\n",
              "      <td>Taken for less than 1 month</td>\n",
              "      <td>For Period Pain:</td>\n",
              "      <td>worked pretty well for my heavy cramps they ar...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>634</th>\n",
              "      <td>634</td>\n",
              "      <td>January 26, 2022</td>\n",
              "      <td>Taken for 2 to 5 years</td>\n",
              "      <td>For Pain:</td>\n",
              "      <td>does not help immediate pain must take for 23 ...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>arthritis, pain</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>635</th>\n",
              "      <td>635</td>\n",
              "      <td>March 22, 2021</td>\n",
              "      <td>Taken for less than 1 month</td>\n",
              "      <td>Aleve (naproxen) for Pain:</td>\n",
              "      <td>this medicine does nothing for my pain</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pain</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>636</th>\n",
              "      <td>636</td>\n",
              "      <td>February 3, 2021</td>\n",
              "      <td>NaN</td>\n",
              "      <td>For Period Pain:</td>\n",
              "      <td>naproxen is the best thing to relieve back pai...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>back pain, pain</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3478 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb8e7ceb-091d-4adc-a9b6-99344c1d0b52')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fb8e7ceb-091d-4adc-a9b6-99344c1d0b52 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fb8e7ceb-091d-4adc-a9b6-99344c1d0b52');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text preprocessing and ADR detection on training data. Please run only if ADRS have not been detected already"
      ],
      "metadata": {
        "id": "XqkOSVmPNUa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.Rating=df.Rating.astype(str)\n",
        "df['Review'] = df['Review'].apply(preprocesstext)\n",
        "#extracting ADRs\n",
        "df['adr'] = df['Review'].apply(find_adrs)\n",
        "print(type(df['adr'].iloc[0]))"
      ],
      "metadata": {
        "id": "ksH5yzG52F53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d36f49-da7b-44a0-d40c-efbb3824eb7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = df['adr'].apply(is_adrs_present)\n",
        "df"
      ],
      "metadata": {
        "id": "aefZ3-h72PE9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "af8ef2df-3ba8-463d-b593-947fb9459131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Date    Time Taken                                     Condition  \\\n",
              "0      NaN           NaN  Lamictal (lamotrigine) for Bipolar Disorder:   \n",
              "1      NaN           NaN  Lamictal (lamotrigine) for Bipolar Disorder:   \n",
              "2      NaN           NaN  Lamictal (lamotrigine) for Bipolar Disorder:   \n",
              "3      NaN           NaN  Lamictal (lamotrigine) for Bipolar Disorder:   \n",
              "4     Kate  June 4, 2020                                  For Anxiety:   \n",
              "...    ...           ...                                           ...   \n",
              "1547   NaN           NaN                 For Schizoaffective Disorder:   \n",
              "1548   NaN           NaN          For Borderline Personality Disorder:   \n",
              "1549   NaN           NaN                               For Depression:   \n",
              "1550   NaN           NaN          For Borderline Personality Disorder:   \n",
              "1551   NaN           NaN                       Lamictal (lamotrigine):   \n",
              "\n",
              "                                                 Review Rating  \\\n",
              "0     ive tried almost all meds for depressionbipola...    8.0   \n",
              "1     i have been on this drug lamictal total time o...    9.0   \n",
              "2     male 33 years old ive been on lamictal for 4 m...   10.0   \n",
              "3     i had been empty depressed doesnt justify the ...   10.0   \n",
              "4     i never review meds but i feel like im doing a...   10.0   \n",
              "...                                                 ...    ...   \n",
              "1547  im on 300 mg of lamotrigine and i feel really ...   10.0   \n",
              "1548  seems to really help in conjunction with dbt i...    9.0   \n",
              "1549  when i start with 200 mg of lamotrigine daily ...   10.0   \n",
              "1550  it has completely stabilised my moods the only...    9.0   \n",
              "1551  this was awful i felt manic and crazy for a we...    1.0   \n",
              "\n",
              "                                             adr  label  \n",
              "0     constipation, depression, insomnia, nausea      1  \n",
              "1                                                     0  \n",
              "2                                    mood swings      1  \n",
              "3                                     depression      1  \n",
              "4                     anxiety, death, depression      1  \n",
              "...                                          ...    ...  \n",
              "1547                              depressed mood      1  \n",
              "1548                                                  0  \n",
              "1549                                                  0  \n",
              "1550                                                  0  \n",
              "1551                                                  0  \n",
              "\n",
              "[1552 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee58059d-5644-4e1d-bc60-1a0bfeca0e5c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Time Taken</th>\n",
              "      <th>Condition</th>\n",
              "      <th>Review</th>\n",
              "      <th>Rating</th>\n",
              "      <th>adr</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lamictal (lamotrigine) for Bipolar Disorder:</td>\n",
              "      <td>ive tried almost all meds for depressionbipola...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>constipation, depression, insomnia, nausea</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lamictal (lamotrigine) for Bipolar Disorder:</td>\n",
              "      <td>i have been on this drug lamictal total time o...</td>\n",
              "      <td>9.0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lamictal (lamotrigine) for Bipolar Disorder:</td>\n",
              "      <td>male 33 years old ive been on lamictal for 4 m...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>mood swings</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lamictal (lamotrigine) for Bipolar Disorder:</td>\n",
              "      <td>i had been empty depressed doesnt justify the ...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>depression</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Kate</td>\n",
              "      <td>June 4, 2020</td>\n",
              "      <td>For Anxiety:</td>\n",
              "      <td>i never review meds but i feel like im doing a...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>anxiety, death, depression</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1547</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>For Schizoaffective Disorder:</td>\n",
              "      <td>im on 300 mg of lamotrigine and i feel really ...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>depressed mood</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1548</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>For Borderline Personality Disorder:</td>\n",
              "      <td>seems to really help in conjunction with dbt i...</td>\n",
              "      <td>9.0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1549</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>For Depression:</td>\n",
              "      <td>when i start with 200 mg of lamotrigine daily ...</td>\n",
              "      <td>10.0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1550</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>For Borderline Personality Disorder:</td>\n",
              "      <td>it has completely stabilised my moods the only...</td>\n",
              "      <td>9.0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1551</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lamictal (lamotrigine):</td>\n",
              "      <td>this was awful i felt manic and crazy for a we...</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1552 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee58059d-5644-4e1d-bc60-1a0bfeca0e5c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee58059d-5644-4e1d-bc60-1a0bfeca0e5c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee58059d-5644-4e1d-bc60-1a0bfeca0e5c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the preprocessed data with ADRS to save time during repeated execution of the program."
      ],
      "metadata": {
        "id": "gMcxynMQNzTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(path+'enter_file_name_here.csv')"
      ],
      "metadata": {
        "id": "CFSGFMw9Ns9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run the code below if text is preprocessed and ADRS have already been detected. Change \"Rating\" to match the name of column containing text reviews"
      ],
      "metadata": {
        "id": "C5w6w2fKNi5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.astype({\"adr\": str})\n",
        "df = df.astype({\"Rating\": str})\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KorFwGl4LGSq",
        "outputId": "44bac9dd-6e95-423a-cb9b-90eecb692737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0      int64\n",
              "Date           object\n",
              "Time Taken     object\n",
              "Condition      object\n",
              "Review         object\n",
              "Rating        float64\n",
              "adr            object\n",
              "label           int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility Functions for sequence labeling. This code is identical to its counterpart in the Testing section. Change the 'Rating' value in get_tags function to match the column containing the text reviews in your respective dataset."
      ],
      "metadata": {
        "id": "JYR9frMmOtw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bio_taggin(x,adrs,label):\n",
        "    #texts = re.findall(r\"[A-Za-z@#]+|\\S\", x) #splits each word but splits even 1st to '1' and 'st'\n",
        "    texts = x.split()                        #splits each word but keeps 1st as '1st'\n",
        "    if len(adrs)==0 or label==0:        \n",
        "        tags = ['O']*len(texts)\n",
        "        #df = pd.DataFrame(list(zip(texts, tags)),columns=['text','tag'])\n",
        "        #print(df)\n",
        "    else:\n",
        "        tags = ['O']*len(texts)\n",
        "        adrs = adrs.split(',')\n",
        "        adrs = sorted(adrs,key=len)\n",
        "        print(adrs)\n",
        "        for adr in adrs:\n",
        "            alist = adr.split()\n",
        "            #print(alist)\n",
        "            for i in range(0,len(alist)):\n",
        "                for j in range(0,len(texts)):\n",
        "                    if texts[j] == str(alist[i]):\n",
        "                        if i == 0 and tags[j]=='O':\n",
        "                            tags[j] = 'B'\n",
        "                        if i >= 1 and (tags[j-1]=='B' or tags[j-1]=='I'):\n",
        "                          tags[j] = 'I'\n",
        "        #df = pd.DataFrame(list(zip(texts, tags)),columns=['text','tag'])\n",
        "    return list(texts),list(tags)\n",
        "\n",
        "# Labelling BIO tags to sentence words\n",
        "def get_tags(df):\n",
        "  sentences = []\n",
        "  tags = []\n",
        "\n",
        "  for index,row in df.iterrows():\n",
        "      sent,tag = bio_taggin(row['Rating'], row['adr'],row['label'])\n",
        "      \n",
        "      sentences.append(sent)\n",
        "      tags.append(tag)\n",
        "  return sentences, tags\n",
        "\n",
        "def tok_with_labels(sent, text_labels):\n",
        "  '''tokenize and keep labels intact'''\n",
        "  tok_sent = []\n",
        "  labels = []\n",
        "  for word, label in zip(sent, text_labels):\n",
        "    tok_word = tokenizer.tokenize(word)\n",
        "    n_subwords = len(tok_word)\n",
        "\n",
        "    tok_sent.extend(tok_word)\n",
        "    labels.extend([label] * n_subwords)\n",
        "  return tok_sent, labels"
      ],
      "metadata": {
        "id": "EE8Su2bR2UrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the input ids, tags and attention masks for training dataset"
      ],
      "metadata": {
        "id": "C3RFAQItdfNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting individual sentences and its corresponding tags\n",
        "train_sentences, train_tags = get_tags(df)\n",
        "\n",
        "# Getting tokenized texts andn labels\n",
        "# Each element is a tuple of : i) sentence in tokens and ii) BIO tags for each token\n",
        "tok_texts_and_labels = [tok_with_labels(sent, labs) for sent, labs in zip(train_sentences, train_tags)]\n",
        "\n",
        "# Separating tokens and labels (BIO tags)\n",
        "tok_texts = [tok_label_pair[0] for tok_label_pair in tok_texts_and_labels]\n",
        "labels = [tok_label_pair[1] for tok_label_pair in tok_texts_and_labels]\n",
        "'''\n",
        "1. Making Input ids\n",
        "2. Making Labels ids\n",
        "3. Making attention masks\n",
        "'''\n",
        "# 1. Making sentences input ids\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tok_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# 2. Making BIO tag input ids\n",
        "tag_values = ['O', 'B', 'I', 'PAD']\n",
        "#tag_values = list(set(itertools.chain.from_iterable(train_tags)))\n",
        "vocab_len = len(tag_values)\n",
        "tag2idx =  {'O': 0, 'B': 1, 'I': 2, 'PAD': 3}\n",
        "#tag2idx = {t: i for i,t in enumerate(tag_values)}\n",
        "\n",
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "\n",
        "# 3. Making attention masks\n",
        "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n"
      ],
      "metadata": {
        "id": "G1gf9F-P2X_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids=torch.tensor(input_ids)\n",
        "attention_masks=torch.tensor(attention_masks)\n",
        "tags=torch.tensor(tags)"
      ],
      "metadata": {
        "id": "u78fvAdt2cQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Test Data Loader which will be used for validation"
      ],
      "metadata": {
        "id": "PpmrpbtAdn9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = TensorDataset(input_ids, attention_masks, tags)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "w-7hEu8a2gIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_to_ids =  {'O': 0, 'B': 1, 'I': 2, 'PAD': 3}\n",
        "ids_to_labels={0:'O',1:'B',2:'I',3:'PAD'}"
      ],
      "metadata": {
        "id": "pvGqEN0Y2qrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The utility function which returns an array of correct and predicted labels"
      ],
      "metadata": {
        "id": "YrAr5YMvdx_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    losses=[]\n",
        "    correct_predictions=0\n",
        "    normalizer = batch_size * MAX_LEN\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            ids, mask, labels = batch\n",
        "            \n",
        "            #loss, eval_logits = model(input_ids=ids, attention_mask=mask)\n",
        "            outputs,eval_logits = model(ids,mask)\n",
        "        \n",
        "            _,preds = torch.max(outputs,dim=2)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            outputs = outputs.view(-1,outputs.shape[-1])\n",
        "            labels_shaped = labels.view(-1)\n",
        "            loss = loss_fn(outputs,labels_shaped)\n",
        "            losses.append(loss.item())\n",
        "            preds=preds.view(-1)\n",
        "            ids_shaped=ids.view(-1)\n",
        "            #print(preds)\n",
        "            '''sentence=tokenizer.convert_ids_to_tokens(ids_shaped.cpu().numpy())\n",
        "            batch_predictions=[ids_to_labels[id.item()] for id in preds]'''\n",
        "            '''for word,label in zip(sentence,batch_predictions):\n",
        "              if(label=='B'):\n",
        "                print(word+':'+label)'''\n",
        "            \n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += labels.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            '''flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1,1) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            \n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != 3 # shape (batch_size, seq_len)\n",
        "        \n",
        "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)'''\n",
        "            \n",
        "            eval_labels.extend(labels_shaped)\n",
        "            eval_preds.extend(preds)\n",
        "            \n",
        "            #tmp_eval_accuracy = accuracy_score(labels_shaped.cpu().numpy(), outputs.cpu().numpy())\n",
        "            #eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
        "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {np.mean(losses)}\")\n",
        "    print(f\"Validation Accuracy: {correct_predictions.double()/(len(testing_loader)*normalizer)}\")\n",
        "\n",
        "    return labels, predictions"
      ],
      "metadata": {
        "id": "7mG4DZW62uAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Please use this section to load a pretrained neural network if one has already been trained in the past. Edit the parameters passed to load_checkpoint to match the location where the trained network weights are stored. If only the Testing section is being run, kindly run the cell containing utilities related to loading the neural network weights and the cell containing the BioBertNER class."
      ],
      "metadata": {
        "id": "5TDjzPJTd9ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BioBertNER().to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "load_checkpoint(path+'seq_labelling_model_aw_naproxen.pt', model, optimizer)"
      ],
      "metadata": {
        "id": "CbRwxcrA3MJy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a144ee87-22c1-4f06-ddfa-96a7e1658894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from <== /content/drive/My Drive/Project/lamotrigine_seq_labelling_model2.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03840242246224079"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The following two cells are responsible for testing"
      ],
      "metadata": {
        "id": "sB-8iU1Set4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels, predictions = valid(model, test_dataloader)"
      ],
      "metadata": {
        "id": "RLwZuKnq3Pqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9884df29-feb8-4dc0-e9a2-6cd4e39700b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss per 100 evaluation steps: 0.0071427300572395325\n",
            "Validation loss per 100 evaluation steps: 0.027297880847666843\n",
            "Validation loss per 100 evaluation steps: 0.024673102892038015\n",
            "Validation Loss: 0.023791939967454\n",
            "Validation Accuracy: 0.9936138188073393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.confusion_matrix(labels, predictions))\n",
        "print(metrics.classification_report(labels, predictions))"
      ],
      "metadata": {
        "id": "mCR56RZM3Rgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f303cbd3-2eb7-4b32-b944-ebfded59fc01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  7439     19    644      0]\n",
            " [    31    765    209      0]\n",
            " [   475     76 268720     19]\n",
            " [     0      0      0 138963]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.94      0.92      0.93      8102\n",
            "           I       0.89      0.76      0.82      1005\n",
            "           O       1.00      1.00      1.00    269290\n",
            "         PAD       1.00      1.00      1.00    138963\n",
            "\n",
            "    accuracy                           1.00    417360\n",
            "   macro avg       0.96      0.92      0.94    417360\n",
            "weighted avg       1.00      1.00      1.00    417360\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility function to get ADRS for a particular review."
      ],
      "metadata": {
        "id": "I2J1memee07S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_adr_using_model(sent):\n",
        "  #print(tokenizer.tokenize(sent))\n",
        "  ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))\n",
        "  pretok_sent=[]\n",
        "  label_list=[]\n",
        "  pretok_idx=0;\n",
        "  adr_list=[]\n",
        "  curr_label='O'\n",
        "  current_word=''\n",
        "  '''for tok,label in tokens,preds:\n",
        "      if not tok.startswith(\"##\"):\n",
        "        if(curr_label=='B' or curr_label=='I'):\n",
        "          adr_list.append(current_word)\n",
        "        current_word=''\n",
        "        current_word+=tok\n",
        "        curr_label=label\n",
        "      else:\n",
        "        current_word += tok[2:]\n",
        "        if(label=='B' or label=='I'):\n",
        "          curr_label=label'''\n",
        "  pretok_sent = pretok_sent[1:]\n",
        "  #print(ids)\n",
        "  input_ids = pad_sequences([ids],\n",
        "                            maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                            truncating=\"post\", padding=\"post\")\n",
        "  #print(input_ids)\n",
        "  input_ids_flattened=input_ids.flatten()\n",
        "  input_ids_flattened=input_ids_flattened.tolist()\n",
        "  token_sent=tokenizer.convert_ids_to_tokens(input_ids_flattened)\n",
        "  #print(token_sent)\n",
        "  attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
        "  #print(attention_masks)\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  ids = torch.from_numpy(input_ids).int().to(device)\n",
        "  mask = torch.from_numpy(attention_masks).int().to(device)\n",
        "  # forward pass\n",
        "  outputs,y_hat = model(ids,mask)\n",
        "  _,preds = torch.max(outputs,dim=2)\n",
        "  #print(preds)\n",
        "  batch_predictions=[ids_to_labels[id.item()] for id in preds.view(-1)]\n",
        "  preds=batch_predictions\n",
        "  #print(batch_predictions)\n",
        "  for tok,label in zip(token_sent,preds):\n",
        "      if not tok.startswith(\"##\"):\n",
        "        if(curr_label=='B' or curr_label=='I'):\n",
        "          adr_list.append(current_word+'-'+curr_label)\n",
        "        current_word=''\n",
        "        current_word+=tok\n",
        "        curr_label=label\n",
        "      else:\n",
        "        current_word += tok[2:]\n",
        "        if(label=='B' or label=='I'):\n",
        "          curr_label=label\n",
        "  #print(adr_list)\n",
        "  return(adr_list)"
      ],
      "metadata": {
        "id": "uI-9m9K13To9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the predictions for the test data"
      ],
      "metadata": {
        "id": "Zjlwcq3hfMfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = [get_adr_using_model(x) for x in df['Review']]\n",
        "df['adr_predicted']=result\n",
        "df"
      ],
      "metadata": {
        "id": "9jn2E21U3ZEB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "48ff3314-521d-44cb-fa0c-2026583e7c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0                Date                   Time Taken  \\\n",
              "0             0                 NaN                          NaN   \n",
              "1             1                 NaN                          NaN   \n",
              "2             2                 NaN                          NaN   \n",
              "3             3                 NaN                          NaN   \n",
              "4             4                Kate                 June 4, 2020   \n",
              "..          ...                 ...                          ...   \n",
              "632         632     August 19, 2021      Taken for 5 to 10 years   \n",
              "633         633  September 23, 2021  Taken for less than 1 month   \n",
              "634         634    January 26, 2022       Taken for 2 to 5 years   \n",
              "635         635      March 22, 2021  Taken for less than 1 month   \n",
              "636         636    February 3, 2021                          NaN   \n",
              "\n",
              "                                        Condition  \\\n",
              "0    Lamictal (lamotrigine) for Bipolar Disorder:   \n",
              "1    Lamictal (lamotrigine) for Bipolar Disorder:   \n",
              "2    Lamictal (lamotrigine) for Bipolar Disorder:   \n",
              "3    Lamictal (lamotrigine) for Bipolar Disorder:   \n",
              "4                                    For Anxiety:   \n",
              "..                                            ...   \n",
              "632                                     For Pain:   \n",
              "633                              For Period Pain:   \n",
              "634                                     For Pain:   \n",
              "635                    Aleve (naproxen) for Pain:   \n",
              "636                              For Period Pain:   \n",
              "\n",
              "                                                Review  Rating  \\\n",
              "0    ive tried almost all meds for depressionbipola...     8.0   \n",
              "1    i have been on this drug lamictal total time o...     9.0   \n",
              "2    male 33 years old ive been on lamictal for 4 m...    10.0   \n",
              "3    i had been empty depressed doesnt justify the ...    10.0   \n",
              "4    i never review meds but i feel like im doing a...    10.0   \n",
              "..                                                 ...     ...   \n",
              "632  my dose is 500 mg twice daily for chronic pain...    10.0   \n",
              "633  worked pretty well for my heavy cramps they ar...     9.0   \n",
              "634  does not help immediate pain must take for 23 ...     2.0   \n",
              "635             this medicine does nothing for my pain     1.0   \n",
              "636  naproxen is the best thing to relieve back pai...     9.0   \n",
              "\n",
              "                                            adr  label  \\\n",
              "0    constipation, depression, insomnia, nausea      1   \n",
              "1                                           nan      0   \n",
              "2                                   mood swings      1   \n",
              "3                                    depression      1   \n",
              "4                    anxiety, death, depression      1   \n",
              "..                                          ...    ...   \n",
              "632                          chronic pain, pain      1   \n",
              "633                                         nan      0   \n",
              "634                             arthritis, pain      1   \n",
              "635                                        pain      1   \n",
              "636                             back pain, pain      1   \n",
              "\n",
              "                        adr_predicted  \n",
              "0    [insomnia, nausea, constipation]  \n",
              "1                                  []  \n",
              "2                      [mood, swings]  \n",
              "3                        [depression]  \n",
              "4        [anxiety, depression, death]  \n",
              "..                                ...  \n",
              "632                   [chronic, pain]  \n",
              "633                                []  \n",
              "634                 [pain, arthritis]  \n",
              "635                            [pain]  \n",
              "636                      [back, pain]  \n",
              "\n",
              "[3478 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-efce0791-de52-4799-a2ef-025531e822e5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Date</th>\n",
              "      <th>Time Taken</th>\n",
              "      <th>Condition</th>\n",
              "      <th>Review</th>\n",
              "      <th>Rating</th>\n",
              "      <th>adr</th>\n",
              "      <th>label</th>\n",
              "      <th>adr_predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lamictal (lamotrigine) for Bipolar Disorder:</td>\n",
              "      <td>ive tried almost all meds for depressionbipola...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>constipation, depression, insomnia, nausea</td>\n",
              "      <td>1</td>\n",
              "      <td>[insomnia, nausea, constipation]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lamictal (lamotrigine) for Bipolar Disorder:</td>\n",
              "      <td>i have been on this drug lamictal total time o...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>nan</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lamictal (lamotrigine) for Bipolar Disorder:</td>\n",
              "      <td>male 33 years old ive been on lamictal for 4 m...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>mood swings</td>\n",
              "      <td>1</td>\n",
              "      <td>[mood, swings]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lamictal (lamotrigine) for Bipolar Disorder:</td>\n",
              "      <td>i had been empty depressed doesnt justify the ...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>depression</td>\n",
              "      <td>1</td>\n",
              "      <td>[depression]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Kate</td>\n",
              "      <td>June 4, 2020</td>\n",
              "      <td>For Anxiety:</td>\n",
              "      <td>i never review meds but i feel like im doing a...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>anxiety, death, depression</td>\n",
              "      <td>1</td>\n",
              "      <td>[anxiety, depression, death]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>632</th>\n",
              "      <td>632</td>\n",
              "      <td>August 19, 2021</td>\n",
              "      <td>Taken for 5 to 10 years</td>\n",
              "      <td>For Pain:</td>\n",
              "      <td>my dose is 500 mg twice daily for chronic pain...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>chronic pain, pain</td>\n",
              "      <td>1</td>\n",
              "      <td>[chronic, pain]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633</th>\n",
              "      <td>633</td>\n",
              "      <td>September 23, 2021</td>\n",
              "      <td>Taken for less than 1 month</td>\n",
              "      <td>For Period Pain:</td>\n",
              "      <td>worked pretty well for my heavy cramps they ar...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>nan</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>634</th>\n",
              "      <td>634</td>\n",
              "      <td>January 26, 2022</td>\n",
              "      <td>Taken for 2 to 5 years</td>\n",
              "      <td>For Pain:</td>\n",
              "      <td>does not help immediate pain must take for 23 ...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>arthritis, pain</td>\n",
              "      <td>1</td>\n",
              "      <td>[pain, arthritis]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>635</th>\n",
              "      <td>635</td>\n",
              "      <td>March 22, 2021</td>\n",
              "      <td>Taken for less than 1 month</td>\n",
              "      <td>Aleve (naproxen) for Pain:</td>\n",
              "      <td>this medicine does nothing for my pain</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pain</td>\n",
              "      <td>1</td>\n",
              "      <td>[pain]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>636</th>\n",
              "      <td>636</td>\n",
              "      <td>February 3, 2021</td>\n",
              "      <td>NaN</td>\n",
              "      <td>For Period Pain:</td>\n",
              "      <td>naproxen is the best thing to relieve back pai...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>back pain, pain</td>\n",
              "      <td>1</td>\n",
              "      <td>[back, pain]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3478 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-efce0791-de52-4799-a2ef-025531e822e5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-efce0791-de52-4799-a2ef-025531e822e5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-efce0791-de52-4799-a2ef-025531e822e5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing the results as a csv file. Change the name as per requirements."
      ],
      "metadata": {
        "id": "e54I3GfffVv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yqVSnSvpfVt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(path+'predicted_results.csv')"
      ],
      "metadata": {
        "id": "VMY3Sk843bZP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}